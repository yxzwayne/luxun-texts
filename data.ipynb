{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get All Book Links\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def is_valid_format(s):\n",
    "    pattern = r'^\\d{3}\\.htm$'\n",
    "    return bool(re.match(pattern, s))\n",
    "\n",
    "\n",
    "root = \"https://zh.wikisource.org/wiki/Author:%E9%AD%AF%E8%BF%85\"\n",
    "\n",
    "root_soup = BeautifulSoup(requests.get(root).text, 'html.parser')\n",
    "all_books = root_soup.find_all(\"li\")\n",
    "all_book_links = []\n",
    "for book_link in all_books:\n",
    "    a = book_link.find(\"a\")\n",
    "    if a and \"wiki/\" in a.get(\"href\") and \":\" not in a.get(\"href\") and len(a.get(\"href\")) > 12 and a.get(\"href\") != root:\n",
    "        all_book_links.append((a.get_text(), a.get(\"href\")))\n",
    "\n",
    "fn = \"luxun-all.txt\"\n",
    "\n",
    "if os.path.exists(fn):\n",
    "    os.remove(fn)\n",
    "\n",
    "open(fn, \"a\").close()\n",
    "\n",
    "for book_name, book_url in all_book_links:\n",
    "    print(\"\\r书名: \" + book_name + \" - \" + book_url)\n",
    "    url = \"https://zh.wikisource.org\" + book_url\n",
    "\n",
    "    # Use requests to fetch the webpage content\n",
    "    while True:\n",
    "        try:\n",
    "            response = requests.get(url, timeout=2)\n",
    "            break\n",
    "        except requests.exceptions.ConnectionError:\n",
    "            print(\"Request timed out, retrying.\")\n",
    "            time.sleep(1)\n",
    "        except requests.exceptions.ReadTimeout:\n",
    "            print(\"Reading timed out, retrying.\")\n",
    "            time.sleep(1)\n",
    "\n",
    "    # Use BeautifulSoup to parse the HTML content\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    lis = soup.find_all('li')\n",
    "    links = []\n",
    "\n",
    "    for li in lis:\n",
    "        a = li.find('a')\n",
    "        if a:\n",
    "            href = a.get('href')\n",
    "            if a and 'wiki/' in href and \":\" not in href and len(href) > 12 and href != url:\n",
    "                links.append((a.get_text(), a.get('href')))\n",
    "\n",
    "\n",
    "    for link_text, link_href in links:\n",
    "        if \"wiki\" in link_href and \"作品\" != link_text and \"阅读\" != link_text:\n",
    "            print(\"\\r\" + link_text + \" - \" + 'https://zh.wikisource.org' + link_href)\n",
    "            # Use requests to fetch the sub page\n",
    "\n",
    "            while True:\n",
    "                try:\n",
    "                    response = requests.get('https://zh.wikisource.org' + link_href, timeout=2)\n",
    "                    break\n",
    "                except requests.exceptions.ConnectionError:\n",
    "                    print(\"Request timed out, retrying.\")\n",
    "                    time.sleep(1)\n",
    "                except requests.exceptions.ReadTimeout:\n",
    "                    print(\"Reading timed out, retrying.\")\n",
    "                    time.sleep(1)\n",
    "\n",
    "            # Use BeautifulSoup to parse the HTML content\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            # Get all paragraph elements\n",
    "            paragraphs = soup.find_all('p')\n",
    "\n",
    "            # Initialize a variable to store the text\n",
    "            text = \"\"\n",
    "\n",
    "            # Iterate over the paragraphs and get their text\n",
    "            for p in paragraphs:\n",
    "                text += p.get_text().strip() + \"\\n\"\n",
    "\n",
    "            # If no text is found in the paragraphs, look for text in <dl><dd></dl> elements\n",
    "            if not text.strip():\n",
    "                dds = soup.find_all('dd')\n",
    "                for dd in dds:\n",
    "                    text += dd.get_text().strip() + \"\\n\"\n",
    "\n",
    "            if not text.strip():\n",
    "                pre = soup.find_all('pre')\n",
    "                for p in pre:\n",
    "                    text += p.get_text().strip() + \"\\n\"\n",
    "\n",
    "            # Write the text to the file\n",
    "            with open(\"luxun-all.txt\", \"a\", encoding=\"utf-8\") as f:\n",
    "                text = text.replace(\"这部作品也可能在本國本地版權期限更長，但對外國外地作品應用較短期限規則的國家以及地区，屬於公有領域。\", \"\")\n",
    "                f.write(\"\\n\" + text + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check new sites posts separately\n",
    "\n",
    "from rich.progress import Progress\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def is_valid_format(s):\n",
    "    pattern = r'^\\d{3}\\.htm$'\n",
    "    return bool(re.match(pattern, s))\n",
    "\n",
    "\n",
    "root = \"https://www.marxists.org/chinese/reference-books/luxun/24/\"\n",
    "\n",
    "url_list = \"\"\"\n",
    "· <a href=\"002.htm\">中国地质略论</a><br>\n",
    "· <a href=\"003.htm\">破恶声论</a><br>\n",
    "· <a href=\"004.htm\">《越铎》出世辞</a><br>\n",
    "· <a href=\"005.htm\">辛亥游录</a><br>\n",
    "· <a href=\"006.htm\">○播布美术意见书</a><br>\n",
    "· <a href=\"007.htm\">《大云寺弥勒重阁碑》校记</a><br>\n",
    "· <a href=\"008.htm\">关于废止《教育纲要》的签注</a><br>\n",
    "· <a href=\"009.htm\">会稽禹庙窆石考</a><br>\n",
    "· <a href=\"010.htm\">《A肱墓志》考</a><br>\n",
    "· <a href=\"011.htm\">《徐法智墓志》考</a><br>\n",
    "· <a href=\"012.htm\">《郑季宣残碑》考</a><br>\n",
    "· <a href=\"013.htm\">《吕超墓志铭》跋</a><br>\n",
    "· <a href=\"014.htm\">《墨经正文》重阅后记</a><br>\n",
    "· <a href=\"015.htm\">《鲍明远集》校记</a><br>\n",
    "· <a href=\"016.htm\">随感录</a><br>\n",
    "· <a href=\"017.htm\">拳术与拳匪</a><br>\n",
    "· <a href=\"018.htm\">他</a><br>\n",
    "· <a href=\"019.htm\">寸铁</a><br>\n",
    "· <a href=\"020.htm\">自言自语</a><br>\n",
    "· <a href=\"021.htm\">“生降死不降”</a><br>\n",
    "· <a href=\"022.htm\">名字</a><br>\n",
    "· <a href=\"023.htm\">无题</a><br>\n",
    "· <a href=\"024.htm\">《遂初堂书目》抄校说明</a><br>\n",
    "· <a href=\"025.htm\">破《唐人说荟》</a><br>\n",
    "· <a href=\"026.htm\">关于《小说世界》</a><br>\n",
    "· <a href=\"027.htm\">看了魏建功君的《不敢盲从》以后的几句声明</a><br>\n",
    "· <a href=\"028.htm\">答广东新会吕蓬尊君</a><br>\n",
    "· <a href=\"029.htm\">对于“笑话”的笑话</a><br>\n",
    "· <a href=\"030.htm\">奇怪的日历</a><br>\n",
    "· <a href=\"031.htm\">笞二百系笞一百之误</a><br>\n",
    "· <a href=\"032.htm\">文学救国法</a><br>\n",
    "· <a href=\"033.htm\">通讯（复孙伏园）</a><br>\n",
    "· <a href=\"034.htm\">为北京女师大学生拟呈教育部文二件</a><br>\n",
    "· <a href=\"035.htm\">《中国小说史略》再版附识</a><br>\n",
    "· <a href=\"036.htm\">《走到出版界》的“战略”</a><br>\n",
    "· <a href=\"037.htm\">《绛洞花主》小引</a><br>\n",
    "· <a href=\"038.htm\">新的世故</a><br>\n",
    "· <a href=\"039.htm\">中山大学开学致语</a><br>\n",
    "· <a href=\"040.htm\">庆祝沪宁克复的那一边</a><br>\n",
    "· <a href=\"041.htm\">关于小说目录两件</a><br>\n",
    "· <a href=\"042.htm\">书苑折枝</a><br>\n",
    "· <a href=\"043.htm\">书苑折枝（二）</a><br>\n",
    "· <a href=\"044.htm\">书苑折枝（三）</a><br>\n",
    "· <a href=\"045.htm\">关于知识阶级</a><br>\n",
    "· <a href=\"046.htm\">补救世道文件四种</a><br>\n",
    "· <a href=\"047.htm\">《丙和甲》按语</a><br>\n",
    "· <a href=\"048.htm\">《某报剪注》按语</a><br>\n",
    "· <a href=\"049.htm\">《“行路难”》按语</a><br>\n",
    "· <a href=\"050.htm\">《禁止标点符号》按语</a><br>\n",
    "· <a href=\"051.htm\">季廉来信按语</a><br>\n",
    "· <a href=\"052.htm\">《示众》编者注</a><br>\n",
    "· <a href=\"053.htm\">通信（复张孟闻）</a><br>\n",
    "· <a href=\"054.htm\">《这回是第三次》按语</a><br>\n",
    "· <a href=\"055.htm\">复晓真、康嗣群</a><br>\n",
    "· <a href=\"056.htm\">《剪报一斑》拾遗</a><br>\n",
    "· <a href=\"057.htm\">《我也来谈谈复旦大学》文后附白</a><br>\n",
    "· <a href=\"058.htm\">通信（复章达生）</a><br>\n",
    "· <a href=\"059.htm\">关于“粗人”</a><br>\n",
    "· <a href=\"060.htm\">《东京通信》按语</a><br>\n",
    "· <a href=\"061.htm\">敬贺新禧</a><br>\n",
    "· <a href=\"062.htm\">致《近代美术史潮论》的读者诸君</a><br>\n",
    "· <a href=\"063.htm\">关于《子见南子》</a><br>\n",
    "· <a href=\"064.htm\">柳无忌来信按语</a><br>\n",
    "· <a href=\"065.htm\">《文艺研究》例言</a><br>\n",
    "· <a href=\"066.htm\">鲁迅自传</a><br>\n",
    "· <a href=\"067.htm\">题赠冯蕙熹</a><br>\n",
    "· <a href=\"068.htm\">《铁甲列车Ｎｒ．14－69》译本后记</a><br>\n",
    "· <a href=\"069.htm\">题《陶元庆的出品》</a><br>\n",
    "· <a href=\"070.htm\">凯绥·珂勒惠支木刻《牺牲》说明</a><br>\n",
    "· <a href=\"071.htm\">《勇敢的约翰》校后记</a><br>\n",
    "· <a href=\"072.htm\">理惠拉壁画《贫人之夜》说明</a><br>\n",
    "· <a href=\"073.htm\">“日本研究”之外</a><br>\n",
    "· <a href=\"074.htm\">介绍德国作家版画展</a><br>\n",
    "· <a href=\"075.htm\">德国作家版画展延期举行真像</a><br>\n",
    "· <a href=\"076.htm\">水灾即“建国”</a><br>\n",
    "· <a href=\"077.htm\">题《外套》</a><br>\n",
    "· <a href=\"078.htm\">我对于《文新》的意见</a><br>\n",
    "· <a href=\"079.htm\">题记一篇</a><br>\n",
    "· <a href=\"080.htm\">文摊秘诀十条</a><br>\n",
    "· <a href=\"081.htm\">闻小林同志之死</a><br>\n",
    "· <a href=\"082.htm\">通信（复魏猛克）</a><br>\n",
    "· <a href=\"083.htm\">我的种痘</a><br>\n",
    "· <a href=\"084.htm\">辩“文人无行”</a><br>\n",
    "· <a href=\"085.htm\">娘儿们也不行</a><br>\n",
    "· <a href=\"086.htm\">自传</a><br>\n",
    "· <a href=\"087.htm\">《无名木刻集》序</a><br>\n",
    "· <a href=\"088.htm\">《玄武湖怪人》按语</a><br>\n",
    "· <a href=\"089.htm\">《〈母亲〉木刻十四幅》序</a><br>\n",
    "· <a href=\"090.htm\">题《淞隐漫录》</a><br>\n",
    "· <a href=\"091.htm\">题《淞隐续录》残本</a><br>\n",
    "· <a href=\"092.htm\">题《漫游随录图记》残本</a><br>\n",
    "· <a href=\"093.htm\">题《风筝误》</a><br>\n",
    "· <a href=\"094.htm\">《译文》创刊号前记</a><br>\n",
    "· <a href=\"095.htm\">做“杂文”也不易</a><br>\n",
    "· <a href=\"096.htm\">题《芥子园画谱三集》赠许广平</a><br>\n",
    "· <a href=\"097.htm\">势所必至，理有固然</a><br>\n",
    "· <a href=\"098.htm\">《中国新文学大系》小说二集编选感想</a><br>\n",
    "· <a href=\"099.htm\">“骗月亮”</a><br>\n",
    "· <a href=\"100.htm\">“某”字的第四义</a><br>\n",
    "· <a href=\"101.htm\">“天生蛮性”</a><br>\n",
    "· <a href=\"102.htm\">死所</a><br>\n",
    "· <a href=\"103.htm\">中国的科学资料</a><br>\n",
    "· <a href=\"104.htm\">“有不为斋”</a><br>\n",
    "· <a href=\"105.htm\">两种“黄帝子孙”</a><br>\n",
    "· <a href=\"106.htm\">聚“珍”</a><br>\n",
    "· <a href=\"107.htm\">《远方》按语</a><br>\n",
    "· <a href=\"108.htm\">题曹白所刻像</a><br>\n",
    "· <a href=\"109.htm\">“中国杰作小说”小引</a><br>\n",
    "· <a href=\"110.htm\">题《凯绥·珂勒惠支版画选集》赠季皦</a><br>\n",
    "· <a href=\"111.htm\">答世界社信</a><br>\n",
    "· <a href=\"112.htm\">关于许绍棣叶溯中黄萍荪</a><br>\n",
    "<br>\n",
    "<div class=\"a2\">\n",
    "<h5 style=\"text-align: center\">附录一</h5></div>\n",
    "<br>\n",
    "· <a href=\"113.htm\">《劲草》译本序（残稿）</a><br>\n",
    "· <a href=\"114.htm\">周豫才告白</a><br>\n",
    "· <a href=\"115.htm\">生理实验术要略</a><br>\n",
    "· <a href=\"116.htm\">什么话？</a><br>\n",
    "· <a href=\"117.htm\">《坏孩子》附记</a><br>\n",
    "· <a href=\"118.htm\">《苦闷的象征》广告</a><br>\n",
    "· <a href=\"119.htm\">《未名丛刊》是什么，要怎样？</a><br>\n",
    "· <a href=\"120.htm\">白事</a><br>\n",
    "· <a href=\"121.htm\">鲁迅启事</a><br>\n",
    "· <a href=\"122.htm\">《莽原》出版预告</a><br>\n",
    "· <a href=\"123.htm\">对于北京女子师范大学风潮宣言</a><br>\n",
    "· <a href=\"124.htm\">编者附白</a><br>\n",
    "· <a href=\"125.htm\">《敏捷的译者》附记</a><br>\n",
    "· <a href=\"126.htm\">正误</a><br>\n",
    "· <a href=\"127.htm\">本刊小信</a><br>\n",
    "· <a href=\"128.htm\">关于《近代美术史潮论》插图</a><br>\n",
    "· <a href=\"129.htm\">编者附白</a><br>\n",
    "· <a href=\"130.htm\">谨启</a><br>\n",
    "· <a href=\"131.htm\">开给许世瑛的书单</a><br>\n",
    "· <a href=\"132.htm\">鲁迅启事</a><br>\n",
    "· <a href=\"133.htm\">《毁灭》和《铁流》的出版预告</a><br>\n",
    "· <a href=\"134.htm\">三闲书屋校印书籍</a><br>\n",
    "· <a href=\"135.htm\">三闲书屋印行文艺书籍</a><br>\n",
    "· <a href=\"136.htm\">《〈铁流〉图》特价告白</a><br>\n",
    "· <a href=\"137.htm\">更正</a><br>\n",
    "· <a href=\"138.htm\">《引玉集》广告</a><br>\n",
    "· <a href=\"139.htm\">《木刻纪程》告白</a><br>\n",
    "· <a href=\"140.htm\">给《戏》周刊编者的订正信</a><br>\n",
    "· <a href=\"141.htm\">《十竹斋笺谱》翻印说明</a><br>\n",
    "· <a href=\"142.htm\">《俄罗斯的童话》</a><br>\n",
    "· <a href=\"143.htm\">给《译文》编者订正的信</a><br>\n",
    "· <a href=\"144.htm\">“三十年集”编目二种</a><br>\n",
    "· <a href=\"145.htm\">《死魂灵百图》广告</a><br>\n",
    "· <a href=\"146.htm\">《凯绥·珂勒惠支版画选集》出版说明</a><br>\n",
    "· <a href=\"147.htm\">《海上述林》上卷插图正误</a><br>\n",
    "<br>\n",
    "<div class=\"a2\">\n",
    "<h5 style=\"text-align: center\">附录二</h5></div>\n",
    "<br>\n",
    "· <a href=\"148.htm\">戛剑生杂记</a><br>\n",
    "· <a href=\"149.htm\">莳花杂志</a><br>\n",
    "· <a href=\"150.htm\">别诸弟三首庚子二月</a><br>\n",
    "· <a href=\"151.htm\">莲蓬人</a><br>\n",
    "· <a href=\"152.htm\">庚子送灶即事</a><br>\n",
    "· <a href=\"153.htm\">祭书神文</a><br>\n",
    "· <a href=\"154.htm\">别诸弟三首辛丑二月　并跋</a><br>\n",
    "· <a href=\"155.htm\">惜花四律步湘州藏春园主人元韵</a><br>\n",
    "· <a href=\"156.htm\">题照赠仲弟</a><br>\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "url_list = url_list.split(\"\\n\")\n",
    "\n",
    "with Progress() as progress:\n",
    "    task = progress.add_task(\"[cyan]Parsing...\", total=len(url_list))\n",
    "\n",
    "    for url in url_list:\n",
    "        if \"href\" and \"htm\" in url:\n",
    "            parse_url = root + url.split(\"\\\"\")[1]\n",
    "            while True:\n",
    "                try:\n",
    "                    response = requests.get(parse_url)\n",
    "                    progress.update(task, advance=1)\n",
    "                    break\n",
    "                except requests.exceptions.ConnectionError:\n",
    "                    print(\"Request timed out, retrying.\")\n",
    "                    time.sleep(1)\n",
    "                except requests.exceptions.ReadTimeout:\n",
    "                    print(\"Reading timed out, retrying.\")\n",
    "                    time.sleep(1)\n",
    "\n",
    "            response.encoding = 'gbk'  # or 'gbk' if 'utf-8' doesn't work\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            # Get all paragraph elements\n",
    "            paragraphs = soup.find_all('p')\n",
    "\n",
    "            # Initialize a variable to store the text\n",
    "            text = \"\"\n",
    "\n",
    "            # Iterate over the paragraphs and get their text\n",
    "            for p in paragraphs:\n",
    "                text += p.get_text().strip() + \"\\n\"\n",
    "\n",
    "            # Write the text to the file\n",
    "            with open(fn, \"a\", encoding=\"utf-8\") as f:\n",
    "                f.write(\"\\n\" + text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "with open(fn, \"r\", encoding=\"utf-8\") as file:\n",
    "    text = file.read()\n",
    "\n",
    "cleaned_text = re.sub(r'〔\\d+〕', '', text)\n",
    "cleaned_text = re.sub(r'(\\d+)', '', text)\n",
    "\n",
    "with open(fn, \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "\n",
    "\n",
    "def normalize_fullwidth_chars(text):\n",
    "    return unicodedata.normalize(\"NFKC\", text)\n",
    "\n",
    "\n",
    "# Read the file\n",
    "with open(fn, \"r\", encoding=\"utf-8\") as file:\n",
    "    text = file.read()\n",
    "\n",
    "# Normalize the text\n",
    "normalized_text = normalize_fullwidth_chars(text)\n",
    "\n",
    "# Write the normalized text back to the file\n",
    "with open(fn, \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(normalized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(fn, \"r\", encoding=\"utf-8\") as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "with open(fn, \"w\", encoding=\"utf-8\") as file:\n",
    "    for line in lines:\n",
    "        if not line.strip().startswith(\"本篇最初\"):\n",
    "            file.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "with open(fn, \"r\", encoding=\"utf-8\") as file:\n",
    "    text = file.read()\n",
    "\n",
    "# Replace more than 3 consecutive new lines with 3 new lines\n",
    "cleaned_text = re.sub(r'\\n{4,}', '\\n\\n\\n', text)\n",
    "\n",
    "with open(fn, \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(cleaned_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
