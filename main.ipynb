{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-29T06:19:38.550306Z",
     "start_time": "2023-05-29T06:19:38.548633Z"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-29T06:19:38.554935Z",
     "start_time": "2023-05-29T06:19:38.552137Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # Specify the URL of the index page and the directory where the texts should be saved.\n",
    "# root = \"https://www.marxists.org/chinese/reference-books/luxun/\"\n",
    "# save_dir = \"raw-text\"\n",
    "#\n",
    "# response = requests.get(root + \"index.htm\")\n",
    "# soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "# # Find all links on the index page.\n",
    "# book_links = soup.find_all(\"a\")\n",
    "# i = 0\n",
    "# for link in book_links:\n",
    "#     book_href = link.get(\"href\")\n",
    "#     # sample content-table url:\n",
    "#     # https://www.marxists.org/chinese/reference-books/luxun/01/000.htm\n",
    "#     # sample actual content url:\n",
    "#     # https://www.marxists.org/chinese/reference-books/luxun/01/001.htm\n",
    "#     book_content = []\n",
    "#\n",
    "#     if \".htm\" in book_href and \"index\" not in book_href and \"/\" in book_href:\n",
    "#         book_content_url_set = set()\n",
    "#         book_num = book_href.split(\"/\")[0]\n",
    "#\n",
    "#         # === book-level extraction === #\n",
    "#         # It just so happened that 000 is the content page in our case.\n",
    "#         soup = BeautifulSoup(requests.get(root+book_href).content, \"html.parser\")\n",
    "#         article_links_of_this_book = soup.find_all(\"a\")\n",
    "#\n",
    "#         for article_link in article_links_of_this_book:\n",
    "#             article_url = article_link.get(\"href\")\n",
    "#             if \"index\" not in article_url:\n",
    "#                 print(book_num + \"/\" + article_url)\n",
    "#                 book_content_url_set.add(article_url)\n",
    "#         for bl in book_content_url_set:\n",
    "#             print(bl)\n",
    "#             article_content = BeautifulSoup(requests.get(root+bl).content, \"html.parser\")\n",
    "#             print(article_content)\n",
    "#             i += 1\n",
    "#             if i > 3:\n",
    "#                 break\n",
    "#\n",
    "# def download_text(url, target_dir):\n",
    "#     r = requests.get(url)\n",
    "#     s = BeautifulSoup(r.content, 'html.parser')\n",
    "#\n",
    "#     # The text is contained within p elements.\n",
    "#     text_elements = s.find_all('p')\n",
    "#\n",
    "#     # Extract the text from the p elements and join them into a single string.\n",
    "#     text = ' '.join([p.get_text() for p in text_elements])\n",
    "#\n",
    "#     # Save the text to a file. The file name is the last part of the URL.\n",
    "#     filename = url.split('/')[-1] + '.txt'\n",
    "#     with open(os.path.join(target_dir, filename), 'w', encoding='utf-8') as f:\n",
    "#         f.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-29T06:30:38.782550Z",
     "start_time": "2023-05-29T06:30:37.650498Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "坟（1927年3月）\n",
      "热风（1925年11月）\n",
      "呐喊（1923年8月）\n",
      "彷徨（1926年8月）\n",
      "野草（1927年7月）\n",
      "朝花夕拾（1928年9月）\n",
      "故事新编（1936年）\n",
      "华盖集（1926年8月）\n",
      "华盖集续编（1927年5月）\n",
      "华盖集续编补编（1948年）\n",
      "而已集（1928年10月）\n",
      "三闲集（1932年9月）\n",
      "二心集（1932年）\n",
      "南腔北调集（1934年4月）\n",
      "伪自由书（1933年10月）\n",
      "准风月谈（1934年12月）\n",
      "花边文学（1936年6月）\n",
      "且介亭杂文（1937年7月）\n",
      "且介亭杂文二集（1937年7月）\n",
      "且介亭杂文末编（1937年7月）\n",
      "且介亭杂文附集\n",
      "集外集（1935年5月）\n",
      "集外集拾遗（1938年）\n",
      "集外集拾遗补编（1952年）\n",
      "中国小说史略（1923-1924年）\n",
      "中国小说的历史的变迁（1938年）\n",
      "汉文学史纲要（1938年）\n",
      "古籍序跋集（1938年）\n",
      "译文序跋集（1938年）\n",
      "鲁迅诗集\n"
     ]
    }
   ],
   "source": [
    "# Specify the URL of the index page and the directory where the texts should be saved.\n",
    "root = \"https://www.marxists.org/chinese/reference-books/luxun/\"\n",
    "save_dir = \"raw-text\"\n",
    "\n",
    "response = requests.get(root + \"index.htm\")\n",
    "response.encoding = 'gb2312'  # specify the correct encoding here\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "# Find all links on the index page.\n",
    "book_links = soup.find_all(\"a\")\n",
    "\n",
    "data = {}\n",
    "\n",
    "for bl in book_links:\n",
    "    book_href = bl.get(\"href\")\n",
    "    if \".htm\" in book_href and \"index\" not in book_href and \"/\" in book_href:\n",
    "        book_title = \"\"\n",
    "        if len(bl.text.split(\"・\")) == 2:\n",
    "            book_title = bl.text.split(\"・\")[1].strip()\n",
    "        else:\n",
    "            book_title = bl.text.strip()\n",
    "        print(book_title)\n",
    "        # === book-level extraction === #\n",
    "        # For each book, extract its articles;\n",
    "        # For each article, extract its titles and contents, use {\"article_title\": article_content, ...}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For each book, extract its articles;\n",
    "# For each article, extract its titles and contents, use {\"article_title\": article_content, ...}\n",
    "i = 0\n",
    "link = book_links[3]\n",
    "book_href = link.get(\"href\")\n",
    "# sample content-table url:\n",
    "# https://www.marxists.org/chinese/reference-books/luxun/01/000.htm\n",
    "# sample actual content url:\n",
    "# https://www.marxists.org/chinese/reference-books/luxun/01/001.htm\n",
    "data = {}\n",
    "\n",
    "if \".htm\" in book_href and \"index\" not in book_href and \"/\" in book_href:\n",
    "    book_content_url_set = set()\n",
    "    book_num = book_href.split(\"/\")[0]\n",
    "\n",
    "    # === book-level extraction === #\n",
    "#     # It just so happened that 000 is the content page in our case.\n",
    "    soup = BeautifulSoup(requests.get(root+book_href).content, \"html.parser\")\n",
    "    article_links_of_this_book = soup.find_all(\"a\")\n",
    "\n",
    "    for article_link in article_links_of_this_book:\n",
    "        article_url = article_link.get(\"href\")\n",
    "        if \"index\" not in article_url:\n",
    "            book_content_url_set.add(book_num + \"/\" + article_url)\n",
    "    book_content_url_set = sorted(book_content_url_set)\n",
    "    for article_sub_url in book_content_url_set:\n",
    "        # === article-level extraction === #\n",
    "        response = requests.get(root+article_sub_url)\n",
    "        response.encoding = 'gb2312'  # specify the correct encoding here\n",
    "        article_soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        # article_content = BeautifulSoup(response.content, \"html.parser\").find_all(\"p\")\n",
    "        # title1_elements = soup.find_all(class_='title1')\n",
    "        # left_align_elements = soup.find_all(align='left')\n",
    "\n",
    "        elements = []\n",
    "\n",
    "        for tag in article_soup(True):  # soup(True) returns all tags\n",
    "            if 'class' in tag.attrs and 'title1' in tag['class']:\n",
    "                elements.append(tag)\n",
    "            elif 'align' in tag.attrs and tag['align'] == 'left':\n",
    "                elements.append(tag)\n",
    "        for e in elements:\n",
    "            print(e)\n",
    "            print()\n",
    "\n",
    "        # for content in title1_elements:\n",
    "        #     print(content)\n",
    "\n",
    "        # print(article_content)\n",
    "        # print(\"\\n\\n\")\n",
    "        i += 1\n",
    "        if i > 10:\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
